{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Transfer Learning with TensorFlow Part 2: Fine-tuning\n",
    "\n",
    "In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our own models (even with less data).\n",
    "\n",
    "Now we're going to cover another type of transfer learning: fine-tuning.\n",
    "\n",
    "In **fine-tuning transfer learning** the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.\n",
    "\n",
    "For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n",
    "\n",
    "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-transfer-learning-feature-extraction-vs-fine-tuning.png)\n",
    "*Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What we're going to cover\n",
    "\n",
    "We're going to go through the follow with TensorFlow:\n",
    "\n",
    "- Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n",
    "- Using the Keras Functional API (a differnt way to build models in Keras)\n",
    "- Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n",
    "- Data augmentation (how to make your training dataset more diverse without adding more data)\n",
    "- Running a series of modelling experiments on our Food Vision data\n",
    "  - **Model 0**: a transfer learning model using the Keras Functional API\n",
    "  - **Model 1**: a feature extraction transfer learning model on 1% of the data with data augmentation\n",
    "  - **Model 2**: a feature extraction transfer learning model on 10% of the data with data augmentation\n",
    "  - **Model 3**: a fine-tuned transfer learning model on 10% of the data\n",
    "  - **Model 4**: a fine-tuned transfer learning model on 100% of the data\n",
    "- Introduce the ModelCheckpoint callback to save intermediate training results\n",
    "- Compare model experiments results using TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 14 08:25:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.02              Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070 ...    On  |   00000000:65:00.0 Off |                  N/A |\n",
      "| N/A   42C    P3             11W /   61W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Are we using a GPU?\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating helper functions\n",
    "\n",
    "Throughout your machine learning experiments, you'll likely come across snippets of code you want to use over and over again.\n",
    "\n",
    "For example, a plotting function which plots a model's `history` object (see `plot_loss_curves()` below).\n",
    "\n",
    "You could recreate these functions over and over again.\n",
    "\n",
    "But as you might've guessed, rewritting the same functions becomes tedious.\n",
    "\n",
    "One of the solutions is to store them in a helper script such as [`helper_functions.py`](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py). And then import the necesary functionality when you need it.\n",
    "\n",
    "For example, you might write:\n",
    "\n",
    "```\n",
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "...\n",
    "\n",
    "plot_loss_curves(history)\n",
    "```\n",
    "\n",
    "Let's see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-01-14 08:25:27--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: ‘helper_functions.py’\n",
      "\n",
      "helper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2025-01-14 08:25:27 (1.65 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get helper_functions.py script from course GitHub\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "\n",
    "# Import helper functions we're going to use\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
