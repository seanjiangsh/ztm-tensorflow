{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 Milestone Project 1: ðŸ”ðŸ‘ Food Vision Bigâ„¢\n",
    "\n",
    "In the previous notebook ([transfer learning part 3: scaling up](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/06_transfer_learning_in_tensorflow_part_3_scaling_up.ipynb)) we built Food Vision mini: a transfer learning model which beat the original results of the [Food101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/) with only 10% of the data.\n",
    "\n",
    "But you might be wondering, what would happen if we used all the data?\n",
    "\n",
    "Well, that's what we're going to find out in this notebook!\n",
    "\n",
    "We're going to be building Food Vision Bigâ„¢, using all of the data from the Food101 dataset.\n",
    "\n",
    "Yep. All 75,750 training images and 25,250 testing images.\n",
    "\n",
    "And guess what...\n",
    "\n",
    "This time **we've got the goal of beating [DeepFood](https://www.researchgate.net/publication/304163308_DeepFood_Deep_Learning-Based_Food_Image_Recognition_for_Computer-Aided_Dietary_Assessment)**, a 2016 paper which used a Convolutional Neural Network trained for 2-3 days to achieve 77.4% top-1 accuracy.\n",
    "\n",
    "> ðŸ”‘ **Note:** **Top-1 accuracy** means \"accuracy for the top softmax activation value output by the model\" (because softmax ouputs a value for every class, but top-1 means only the highest one is evaluated). **Top-5 accuracy** means \"accuracy for the top 5 softmax activation values output by the model\", in other words, did the true label appear in the top 5 activation values? Top-5 accuracy scores are usually noticeably higher than top-1.\n",
    "\n",
    "|  | ðŸ”ðŸ‘ Food Vision Bigâ„¢ | ðŸ”ðŸ‘ Food Vision mini |\n",
    "|-----|-----|-----|\n",
    "| Dataset source | TensorFlow Datasets | Preprocessed download from Kaggle | \n",
    "| Train data | 75,750 images | 7,575 images | \n",
    "| Test data | 25,250 images | 25,250 images | \n",
    "| Mixed precision | Yes | No |\n",
    "| Data loading | Performanant tf.data API | TensorFlow pre-built function |  \n",
    "| Target results | 77.4% top-1 accuracy (beat [DeepFood paper](https://arxiv.org/abs/1606.05675)) | 50.76% top-1 accuracy (beat [Food101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf)) | \n",
    "\n",
    "*Table comparing difference between Food Vision Big (this notebook) versus Food Vision mini (previous notebook).*\n",
    "\n",
    "Alongside attempting to beat the DeepFood paper, we're going to learn about two methods to significantly improve the speed of our model training:\n",
    "1. Prefetching\n",
    "2. Mixed precision training\n",
    "\n",
    "But more on these later.\n",
    "\n",
    "## What we're going to cover\n",
    "\n",
    "* Using TensorFlow Datasets to download and explore data\n",
    "* Creating preprocessing function for our data\n",
    "* Batching & preparing datasets for modelling (**making our datasets run fast**)\n",
    "* Creating modelling callbacks\n",
    "* Setting up **mixed precision training**\n",
    "* Building a feature extraction model (see [transfer learning part 1: feature extraction](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb))\n",
    "* Fine-tuning the feature extraction model (see [transfer learning part 2: fine-tuning](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb))\n",
    "* Viewing training results on TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU\n",
    "\n",
    "For this notebook, we're going to be doing something different.\n",
    "\n",
    "We're going to be using mixed precision training.\n",
    "\n",
    "Mixed precision training was introduced in [TensorFlow 2.4.0](https://blog.tensorflow.org/2020/12/whats-new-in-tensorflow-24.html) (a very new feature at the time of writing).\n",
    "\n",
    "What does **mixed precision training** do?\n",
    "\n",
    "Mixed precision training uses a combination of single precision (float32) and half-preicison (float16) data types to speed up model training (up 3x on modern GPUs).\n",
    "\n",
    "We'll talk about this more later on but in the meantime you can read the [TensorFlow documentation on mixed precision](https://www.tensorflow.org/guide/mixed_precision) for more details.\n",
    "\n",
    "For now, before we can move forward if we want to use mixed precision training, we need to make sure the GPU is compataible. \n",
    "\n",
    "For mixed precision training to work, **you need access to a GPU with a compute compability score of 7.0+**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU (UUID: GPU-f2fc91ef-d4e3-c010-ebf9-2d38bc5874e8)\n"
     ]
    }
   ],
   "source": [
    "# Get GPU name\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since mixed precision training was introduced in TensorFlow 2.4.0, make sure you've got at least TensorFlow 2.4.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 06:45:18.281467: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 06:45:22.922393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-28 06:45:24.787049: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-28 06:45:25.222592: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 06:45:28.717021: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 06:45:52.191872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Notebook last run (end-to-end): 2025-02-28 06:46:31.495057\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow version (should be minimum 2.4.0+ but 2.13.0+ is better)\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Add timestamp\n",
    "import datetime\n",
    "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get helper functions\n",
    "\n",
    "We've created a series of helper functions throughout the previous notebooks in the course. Instead of rewriting them (tedious), we'll import the [`helper_functions.py`](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py) file from the GitHub repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 'helper_functions.py' already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Get helper functions file\n",
    "import os \n",
    "\n",
    "if not os.path.exists(\"helper_functions.py\"):\n",
    "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
    "else:\n",
    "    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
    "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TensorFlow Datasets to Download Data\n",
    "\n",
    "In previous notebooks, we've downloaded our food images (from the [Food101 dataset](https://www.kaggle.com/dansbecker/food-101/home)) from Google Storage.\n",
    "\n",
    "And this is a typical workflow you'd use if you're working on your own datasets.\n",
    "\n",
    "However, there's another way to get datasets ready to use with TensorFlow.\n",
    "\n",
    "For many of the most popular datasets in the machine learning world (often referred to and used as benchmarks), you can access them through [TensorFlow Datasets (TFDS)](https://www.tensorflow.org/datasets/overview).\n",
    "\n",
    "What is **TensorFlow Datasets**?\n",
    "\n",
    "A place for prepared and ready-to-use machine learning datasets.\n",
    "\n",
    "Why use TensorFlow Datasets?\n",
    "\n",
    "* Load data already in Tensors\n",
    "* Practice on well established datasets\n",
    "* Experiment with differet data loading techniques (like we're going to use in this notebook)\n",
    "* Experiment with new TensorFlow features quickly (such as mixed precision training)\n",
    "\n",
    "Why *not* use TensorFlow Datasets?\n",
    "\n",
    "* The datasets are static (they don't change, like your real-world datasets would)\n",
    "* Might not be suited for your particular problem (but great for experimenting)\n",
    "\n",
    "To begin using TensorFlow Datasets we can import it under the alias `tfds`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get TensorFlow Datasets\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find all of the available datasets in TensorFlow Datasets, you can use the `list_builders()` method.\n",
    "\n",
    "After doing so, we can check to see if the one we're after (`\"food101\"`) is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 06:46:57.040707: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'food101' in TensorFlow Datasets: True\n"
     ]
    }
   ],
   "source": [
    "# Get all available datasets in TFDS\n",
    "datasets_list = tfds.list_builders()\n",
    "\n",
    "# Set our target dataset and see if it exists\n",
    "target_dataset = \"food101\"\n",
    "print(f\"'{target_dataset}' in TensorFlow Datasets: {target_dataset in datasets_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! It looks like the dataset we're after is available (note there are plenty more available but we're on Food101).\n",
    "\n",
    "To get access to the Food101 dataset from the TFDS, we can use the [`tfds.load()`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) method.\n",
    "\n",
    "In particular, we'll have to pass it a few parameters to let it know what we're after:\n",
    "* `name` (str) : the target dataset (e.g. `\"food101\"`)\n",
    "* `split` (list, optional) : what splits of the dataset we're after (e.g. `[\"train\", \"validation\"]`)\n",
    "  * the `split` parameter is quite tricky. See [the documentation for more](https://github.com/tensorflow/datasets/blob/master/docs/splits.md).\n",
    "* `shuffle_files` (bool) : whether or not to shuffle the files on download, defaults to `False` \n",
    "* `as_supervised` (bool) : `True` to download data samples in tuple format (`(data, label)`) or `False` for dictionary format \n",
    "* `with_info` (bool) : `True` to download dataset metadata (labels, number of samples, etc)\n",
    "\n",
    "> ðŸ”‘ **Note:** Calling the `tfds.load()` method will start to download a target dataset to disk if the `download=True` parameter is set (default). This dataset could be 100GB+, so make sure you have space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data (takes about 5-6 minutes in Google Colab)\n",
    "data_dir = \"data/image_data/food_101\"\n",
    "\n",
    "(train_data, test_data), ds_info = tfds.load(\n",
    "    name=\"food101\",  # target dataset to get from TFDS\n",
    "    split=[\"train\", \"validation\"],  # what splits of data should we get? note: not all datasets have train, valid, test\n",
    "    data_dir=data_dir,  # specify the download directory\n",
    "    as_supervised=True,  # download data in tuple format (sample, label), e.g. (image, label)\n",
    "    with_info=True)  # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! After a few minutes of downloading, we've now got access to entire Food101 dataset (in tensor format) ready for modelling.\n",
    "\n",
    "Now let's get a little information from our dataset, starting with the class names.\n",
    "\n",
    "Getting class names from a TensorFlow Datasets dataset requires downloading the \"`dataset_info`\" variable (by using the `as_supervised=True` parameter in the `tfds.load()` method, **note:** this will only work for supervised datasets in TFDS).\n",
    "\n",
    "We can access the class names of a particular dataset using the `dataset_info.features` attribute and accessing `names` attribute of the the `\"label\"` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'image': Image(shape=(None, None, 3), dtype=uint8),\n",
       "    'label': ClassLabel(shape=(), dtype=int64, num_classes=101),\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Features of Food101 TFDS\n",
    "ds_info.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple_pie',\n",
       " 'baby_back_ribs',\n",
       " 'baklava',\n",
       " 'beef_carpaccio',\n",
       " 'beef_tartare',\n",
       " 'beet_salad',\n",
       " 'beignets',\n",
       " 'bibimbap',\n",
       " 'bread_pudding',\n",
       " 'breakfast_burrito']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names\n",
    "class_names = ds_info.features[\"label\"].names\n",
    "class_names[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
